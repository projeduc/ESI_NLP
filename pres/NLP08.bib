
@book{2019-jurafsky-martin,
	author = {Jurafsky, Dan and Martin, James H.},
	title = {Speech and Language Processing},
	year = {2019},
	note = {\url{https://web.stanford.edu/~jurafsky/slp3/}},
}


@thesis{2017-djemaa,
	author = {Marianne Djemaa},
	title = {Stratégie Domaine Par Domaine Pour
	La Création d'un Framenet Du Français :
	Annotations en Corpus de Cadres et Rôles Sémantiques
	},
	type = {Thèse de doctorat de Linguistique Théorique, Descriptive et Automatique},
	institution = {Université Sorbonne Paris Cité},
	year = {2017},
}

@book{2002-russell-norvig,
	author = {Stuart J. Russell and Peter Norvig},
	title = {Artificial Intelligence: A Modern Approach},
	year = {2002},
	edition={2nd},
	publisher={Prentice Hall},
}

@book{2018-eisenstein,
	author = {Jacob Eisenstein},
	title = {Natural Language Processing},
	year = {2018},
}


@Misc{2020-cmu,
	author = {Alan W. Black and David R. Mortensen},
	title = {Compositional semantics, semantic parsing},
	howpublished = {Cours "Natural Language Processing F20"},
	year = {2020},
	note = {Carnegie Mellon University},
	annote = {\url{http://demo.clab.cs.cmu.edu/NLP/}},
}

@inproceedings{2013-banarescu-al,
	title = "{A}bstract {M}eaning {R}epresentation for Sembanking",
	author = "Banarescu, Laura  and
	Bonial, Claire  and
	Cai, Shu  and
	Georgescu, Madalina  and
	Griffitt, Kira  and
	Hermjakob, Ulf  and
	Knight, Kevin  and
	Koehn, Philipp  and
	Palmer, Martha  and
	Schneider, Nathan",
	booktitle = "Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse",
	month = aug,
	year = "2013",
	address = "Sofia, Bulgaria",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/W13-2322",
	pages = "178--186",
}


@inproceedings{2017-he-al,
	title = "Deep Semantic Role Labeling: What Works and What{'}s Next",
	author = "He, Luheng  and
	Lee, Kenton  and
	Lewis, Mike  and
	Zettlemoyer, Luke",
	booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2017",
	address = "Vancouver, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/P17-1044",
	doi = "10.18653/v1/P17-1044",
	pages = "473--483",
	abstract = "We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on theCoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10{\%} relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results.",
}

@inproceedings{2023-mohammadshahi-henderson,
	title = "Syntax-Aware Graph-to-Graph Transformer for Semantic Role Labelling",
	author = "Mohammadshahi, Alireza  and
	Henderson, James",
	editor = "Can, Burcu  and
	Mozes, Maximilian  and
	Cahyawijaya, Samuel  and
	Saphra, Naomi  and
	Kassner, Nora  and
	Ravfogel, Shauli  and
	Ravichander, Abhilasha  and
	Zhao, Chen  and
	Augenstein, Isabelle  and
	Rogers, Anna  and
	Cho, Kyunghyun  and
	Grefenstette, Edward  and
	Voita, Lena",
	booktitle = "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.repl4nlp-1.15/",
	doi = "10.18653/v1/2023.repl4nlp-1.15",
	pages = "174--186",
	abstract = "Recent models have shown that incorporating syntactic knowledge into the semantic role labelling (SRL) task leads to a significant improvement. In this paper, we propose Syntax-aware Graph-to-Graph Transformer (SynG2G-Tr) model, which encodes the syntactic structure using a novel way to input graph relations as embeddings, directly into the self-attention mechanism of Transformer. This approach adds a soft bias towards attention patterns that follow the syntactic structure but also allows the model to use this information to learn alternative patterns. We evaluate our model on both span-based and dependency-based SRL datasets, and outperform previous alternative methods in both in-domain and out-of-domain settings, on CoNLL 2005 and CoNLL 2009 datasets."
}

@misc{2023-chanin,
	title={Open-source Frame Semantic Parsing}, 
	author={David Chanin},
	year={2023},
	eprint={2303.12788},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2303.12788}, 
}

@misc{2020-kalyanpur,
	title={Open-Domain Frame Semantic Parsing Using Transformers}, 
	author={Aditya Kalyanpur and Or Biran and Tom Breloff and Jennifer Chu-Carroll and Ariel Diertani and Owen Rambow and Mark Sammons},
	year={2020},
	eprint={2010.10998},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2010.10998}, 
}

@misc{2023-lee-amr,
	title={AMR Parsing with Instruction Fine-tuned Pre-trained Language Models}, 
	author={Young-Suk Lee and Ramón Fernandez Astudillo and Radu Florian and Tahira Naseem and Salim Roukos},
	year={2023},
	eprint={2304.12272},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2304.12272}, 
}
